#+TITLE: 判別分析 - 多値化と評価
#+SUBTITLE: 数理科学続論J
#+AUTHOR: 村田 昇
#+EMAIL: noboru.murata@eb.waseda.ac.jp
#+DATE: 2019.12.06
:preamble:
#+INCLUDE: "./myconf.org"
#+STARTUP: hidestars content
# C-c C-x C-v でinlineを切り替え
# <l C-i でlatex block
# C-c '
:end:

* 講義の予定
  - 第1日: 判別分析の考え方
  - *第2日: 多値判別と分析の評価*

* 判別分析の復習
** 判別分析 (discriminant analysis)
   - 個体の特徴量から属するクラスを予測する関係式を構成
   - 関係式: *判別関数* (discriminant function)
     - 特徴量: $X=(X_1,\dots,X_q)$ 
     - クラス: $Y$ ($K(\geq2)$ 個のラベル)
   - 判別関数による分類:
     - 1次式の場合: *線形判別分析* (linear discriminant analysis)
     - 2次式の場合: *2次判別分析* (quadratic discriminant analysis)

** 判別分析の考え方
   - 確率による定式化
     - $X=\boldsymbol{x}$ の下で $Y=k$ となる条件付確率を計算
       # #+begin_export latex
       \begin{equation}
	 p_k(\boldsymbol{x}):=P(Y=k|X=\boldsymbol{x})
       \end{equation}
       # #+end_export
     - 所属する確率が最も高いクラスに個体を分類
   - 観測データ: $n$ 個のデータ $(Y,X_1,\dots,X_q)$ 
     # #+begin_export latex
     \begin{equation}
       \{(y_i,x_{i1},\dots,x_{iq})\}_{i=1}^n
     \end{equation}
     # #+end_export
     から $p_k(\boldsymbol{x})$ を構成
   # - (直接判別基準を構築するアプローチもある．例:サポートベクターマシン)

** 事前確率と事後確率
   - *事前確率*: $\pi_k=P(Y=k)$ (prior probability)
     - $X=\boldsymbol{x}$ が与えられる前に予測されるクラス
   - *事後確率*: $p_k(\boldsymbol{x})$ (posterior probability)
     - $X=\boldsymbol{x}$ が与えられた後に予測されるクラス
   - *条件付確率*: $f_k(\boldsymbol{x})$
     - クラスで条件付けた $X=\boldsymbol{x}$ の確率質量・確率密度
   - ベイズの公式による関係:
     # #+begin_export latex
     \begin{equation}
       p_k(\boldsymbol{x})
       =
       \frac{f_k(\boldsymbol{x})\pi_k}{\sum_{l=1}^Kf_l(\boldsymbol{x})\pi_l}
     \end{equation}
     # #+end_export
     (事前確率が特徴量の確率で重みで変更される)

** 線形判別
   - $f_k(\boldsymbol{x})$ の仮定:
     - $q$ 変量正規分布の密度関数
     - 平均ベクトル $\boldsymbol{\mu}_k$: クラスごとに異なる
     - 共分散行列 $\Sigma$: すべてのクラスで共通
     # #+begin_export latex
     \begin{equation}
       f_k(\boldsymbol{x})
       =
       \frac{1}{(2\pi)^{q/2}\sqrt{\det\Sigma}}
       \exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_k)^\top
	 \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_k)\right)
     \end{equation}
     # #+end_export
   - 線形判別関数 (linear discriminant function)
     # #+begin_export latex
     \begin{equation}
       \delta_k(\boldsymbol{x})
       =
       \boldsymbol{x}^\top\Sigma^{-1}\boldsymbol{\mu}_k
       -\frac{1}{2}\boldsymbol{\mu}_k^\top\Sigma^{-1}\boldsymbol{\mu}_k
       +\log\pi_k
     \end{equation}
     # #+end_export
     ($\boldsymbol{x}$ の1次式)
** 2次判別
   - $f_k(\boldsymbol{x})$ の仮定:
     - $q$ 変量正規分布の密度関数
     - 平均ベクトル $\boldsymbol{\mu}_k$: クラスごとに異なる
     - 共分散行列 $\Sigma_k$: *クラスごとに異なる*
     # #+begin_export latex
     \begin{equation}
       f_k(\boldsymbol{x})
       =
       \frac{1}{(2\pi)^{q/2}\sqrt{\det\Sigma_k}}
       \exp\left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_k)^\top
	 \Sigma_k^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_k)\right)
     \end{equation}
     # #+end_export
   - 2次判別関数
     # #+begin_export latex
     \begin{equation}
       \delta_k(\boldsymbol{x})
       =
       -\frac{1}{2}\det\Sigma_k
       -\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu}_k)^\top
       \Sigma_k^{-1}(\boldsymbol{x}-\boldsymbol{\mu}_k)
       +\log\pi_k
     \end{equation}
     # #+end_export
     ($\boldsymbol{x}$ の2次式)

* 多値判別
** 多値判別の考え方
   - 判別関数の比較 (判別関数 $\delta_k$ を比較)
   - 2値判別の統合 (組み合わせ数 ${}_nC_2$)
   - $k-1$ 個の特徴量への変換 (Rの線形判別)

** 変動の分解
   - 3種類の変動の関係
     - 全変動　: $A=\sum_{i=1}^{n}(\boldsymbol{x}_{i}-\mu)(\boldsymbol{x}_{i}-\mu)^\top$
     - 群内変動: $W=\sum_{i=1}^{n}(\boldsymbol{x}_{i}-\mu_{y_i})(\boldsymbol{x}_{i}-\mu_{y_i})^\top$
     - 群間変動: $B=\sum_{k=1}^{K}n_{k}(\mu_{k}-\mu)(\mu_{k}-\mu)^\top$ \\
       　　　　　　($n_{k}$ はクラス $k$ のデータ数)
     # #+begin_export latex
     \begin{equation}
       \text{全変動}A
       =
       \text{群内変動}W
       +\text{群間変動}B
     \end{equation}
     # #+end_export

** Fisherの線形判別
   - 特徴量 $Z=\alpha^\top X$
   - 良い $Z$ の基準:
     - クラス内では集まっているほど良い
     - クラス間では離れているほど良い
   - Fisherの基準:
     # #+begin_export latex
     \begin{equation}
       \text{maximize}\quad \alpha^\top B\alpha
       \quad\text{s.t.}\quad \alpha^\top W\alpha=\text{const.}
     \end{equation}
     # #+end_export

** Fisherの線形判別の解
   - $\alpha$ は $W^{-1}B$ の最大固有値 (主成分分析の導出と同様)
     - $K=2$ の場合:
       # #+begin_export latex
       \begin{equation}
	 \alpha\propto W^{-1}(\mu_1-\mu_2)
       \end{equation}
       # #+end_export
       (線形判別と一致する)
     - 一般の $K$ の場合: 第1から第 $K-1$ 固有値を用いる
   - 判別方法: 特徴量の距離を用いる
     - $d_{k}=\sum_{l=1}^{K-1}(\alpha_l^\top\boldsymbol{x}-\alpha_l^\top\mu_k)^2$ を計算
     - 最小の $d_{k}$ となるクラス $k$ に判別

** 演習
   :PROPERTIES:
   :reveal_background: #EEEEFF
   :END:
   #+ATTR_HTML: :align center
   [[./example/da-multi.r]] を確認してみよう

* 分析の評価
** 誤り率
   - 単純な誤り:
     # #+begin_export latex
     \begin{equation*}
       \text{(誤り率)}
       =\frac{\text{(誤って判別されたデータ数)}}
       {\text{(全データ数)}}
     \end{equation*}
     # #+end_export
   - 判別したいラベル: 陽性 (positive)
     - 正しく陽性と判定: *真陽性* (true positive; TP)
     - 誤って陽性と判定: *偽陽性* (false positive; FP) (第I種過誤)
     - 誤って陰性と判定: *偽陰性* (false negative; FN) (第II種過誤)
     - 正しく陰性と判定: *真陰性* (true negative; TN) 

** 混同行列 (confusion matrix)
  |------------+-------------------------+-------------------------|
  |            | 真値は陽性              | 真値は陰性              |
  |------------+-------------------------+-------------------------|
  | 判別は陽性 | 真陽性 (True Positive)  | 偽陽性 (False Positive) |
  | 判別は陰性 | 偽陰性 (False Negative) | 真陰性 (True Negative)  |
  |------------+-------------------------+-------------------------|
  (転置で書く流儀もあるので注意)

** 混同行列 (confusion matrix)
   |------------+-------------------------+-------------------------|
   |            | 判別は陽性              | 判別は陰性              |
   |------------+-------------------------+-------------------------|
   | 真値は陽性 | 真陽性 (True Positive)  | 偽陰性 (False Negative) |
   | 真値は陰性 | 偽陽性 (False Positive) | 真陰性 (True Negative)  |
   |------------+-------------------------+-------------------------|
   
#   (パターン認識や機械学習で多く見られた書き方．誤差行列 (error matrix) ともいう)

** いろいろな評価基準
   # #+begin_export latex
   \begin{align}
     \text{(真陽性率)}
     &=\frac{TP}{TP+FN} &&\text{(true positive rate)}\\
     \text{(真陰性率)}
     &=\frac{TN}{FP+TN} &&\text{(true negative rate)}\\
     \text{(適合率)}
     &=\frac{TP}{TP+FP} &&\text{(precision)}\\
     \text{(正答率)}
     &=\frac{TP+TN}{TP+FP+TN+FN} &&\text{(accuracy)}
   \end{align}
   # #+end_export
   - 真陽性率: 感度 (sensitivity) あるいは 再現率 (recall)
   - 真陰性率: 特異度 (specificity)
   - 正答率: 精度

** F-値 (F-measure, F-score)
   # #+begin_export latex
   \begin{align}
     F_{1}&=\frac{2}{{1}/{\text{(再現率)}}+{1}/{\text{(適合率)}}}
     &&\text{(調和平均)}\\
     F_{\beta}&=\frac{\beta^{2}+1}{{\beta^{2}}/{\text{(真陽性率)}}+{1}/{\text{(適合率)}}}
     &&\text{(重み付き調和平均)}
   \end{align}
   # #+end_export
   再現率(真陽性率)と適合率の調和平均
# 	=\frac{2TP}{2TP+FP+FN}

* 訓練誤差と予測誤差
  - *訓練誤差* (training error):
    既知のデータに対する誤り率
  - *予測誤差* (predictive error): 
    未知のデータに対する誤り率
  - 訓練誤差は真の誤り率より良くなることが多い \\
    既知データの判別に特化している可能性があるため
    - 過適応 (over-fitting)
    - 過学習 (over-training)

** 交叉検証
   - 収集したデータを訓練データと試験データに分割して用いる
     - *訓練データ* (training data): 判別関数を構成するための
     - *試験データ* (test data): 予測精度を評価するための
   - データの分割に依存して予測誤差の評価が大きく変わる
   - データ分割の偏りによる精度評価の偏りを避けるために複数回分割を行う

** 交叉検証法 (cross-validation)
   - $k$ -重交叉検証法 ($k$ -fold cross-validation; $k$ -vold CV)
     - $n$ 個のデータを $k$ ブロックにランダムに分割
     - 第 $i$ ブロックを除いた $k-1$ ブロックのデータで判別関数を推定
     - 除いておいた第 $i$ ブロックのデータで予測誤差を評価
     - $i=1,\dotsc,k$ で繰り返し $k$ 個の予測誤差で評価 (平均や分散)
   - leave-one-out法 (leave-one-out cross-validation; LOO-CV)
     - $k=n$ として上記を実行

* Rによる実習
** 演習
   :PROPERTIES:
   :reveal_background: #EEEEFF
   :END:
   #+ATTR_HTML: :align center
   [[./example/da-cv.r]] を確認してみよう

** 演習
   :PROPERTIES:
   :reveal_background: #EEEEFF
   :END:
   #+ATTR_HTML: :align center
   前回用いたデータについて線形・2次どちらの判別方法が望ましいか検証してみよう
